{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4670ab62-ca3c-46e9-be3c-fb788a4a1ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff411842-c4c2-4ff1-98dd-d4f055067761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model_name = \"deepseek-ai/deepseek-llm-1.5b\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60d9bb-bd6e-4195-9007-44b7c661ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8240d67-03e1-4289-883f-471cf072c0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.0, metrics={'train_runtime': 2.8757, 'train_samples_per_second': 2.086, 'train_steps_per_second': 1.043, 'total_flos': 28493906706432.0, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"data.jsonl\"})\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Combine instruction and response (adjust the formatting as needed)\n",
    "    texts = [\n",
    "        f\"Instruction: {instr}\\nResponse: {resp}\"\n",
    "        for instr, resp in zip(examples[\"instruction\"], examples[\"response\"])\n",
    "    ]\n",
    "    # Tokenize the combined text\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # or use another strategy as needed\n",
    "        max_length=512       # adjust maximum length as needed\n",
    "    )\n",
    "    # For causal LM training, it's common to set labels to be the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing to your training split\n",
    "processed_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "# Optionally remove the original columns\n",
    "processed_dataset = processed_dataset.remove_columns([\"instruction\", \"response\"])\n",
    "\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False,\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-deepseek\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"no\",  # note: use 'eval_strategy' as per the warning\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False  # disable removal of columns not in the model's forward signature\n",
    ")\n",
    "\n",
    "\n",
    "# Train model\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b87b0b-48e4-419a-9c11-9e6228522191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e72dc2-7749-4d6c-bb16-d07cf298a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./deepseek-r1-1.5B-qwen-distill-finetuned\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig\n",
    "\n",
    "base_model_id = \"deepseek-r1-1.5B-qwen-distill-finetuned\"  # your base model identifier\n",
    "\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    # Check if the peft_config is a dictionary (typically mapping adapter names to config objects)\n",
    "    if isinstance(model.peft_config, dict):\n",
    "        for adapter_name, adapter_config in model.peft_config.items():\n",
    "            if isinstance(adapter_config, str):\n",
    "                # If the adapter config is a string (likely a path), load it as a proper PeftConfig object.\n",
    "                config_obj = PeftConfig.from_pretrained(adapter_config)\n",
    "                config_obj.base_model_name_or_path = base_model_id\n",
    "                model.peft_config[adapter_name] = config_obj\n",
    "            elif isinstance(adapter_config, dict):\n",
    "                # If it's a dictionary, update it directly.\n",
    "                model.peft_config[adapter_name][\"base_model_name_or_path\"] = base_model_id\n",
    "            else:\n",
    "                # Otherwise, assume it's already an object and set the attribute.\n",
    "                adapter_config.base_model_name_or_path = base_model_id\n",
    "    elif isinstance(model.peft_config, str):\n",
    "        # If peft_config is directly a string, load it as a proper config object.\n",
    "        peft_config = PeftConfig.from_pretrained(model.peft_config)\n",
    "        peft_config.base_model_name_or_path = base_model_id\n",
    "        model.peft_config = peft_config\n",
    "    else:\n",
    "        # Fallback: assume it's already a config object.\n",
    "        model.peft_config.base_model_name_or_path = base_model_id\n",
    "else:\n",
    "    # If there is no peft_config attribute, update the base model name in model.config\n",
    "    model.config.base_model_name_or_path = base_model_id\n",
    "\n",
    "# --- Save the model and tokenizer ---\n",
    "output_dir = \"./deepseek-r1-1.5B-qwen-distill-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d528ef40-0deb-40ce-b553-f9fde66fa3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Specify your base model identifier (this should be the original model you started with)\n",
    "base_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Load the tokenizer and the full base model (this ensures we have a valid config with a \"model_type\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "\n",
    "# Path to your saved adapter (fine-tuned) weights/configuration\n",
    "adapter_path = \"./deepseek-r1-1.5B-qwen-distill-finetuned\"\n",
    "\n",
    "# Load the adapter (PEFT) weights on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846ac48f-b9e4-4aa6-9ebe-cab2d36f7492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: How do I reset my password??\n",
      "Response: To reset your password, you should follow these steps:\n",
      "\n",
      "1. Enter your new password in the password field.\n",
      "2. Click the \"Remember Me\" button.\n",
      "3. Log in to your account.\n",
      "\n",
      "Please note: If you have an account with a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Inference Example ---\n",
    "prompt = \"Instruction: How do I reset my password??\\nResponse:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode and print the generated output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003254c-93fd-4729-bbf9-b194d6321903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
